{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOzl9joMgXTQEq+9tX56+cf",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/andrew3690/F-A_root_project/blob/main/Research_Stocks.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Downloading Modules:\n",
        "Spark, pyspark, Java.\n",
        "\n",
        "Bellow it is the old fashion way, risky not Recomended"
      ],
      "metadata": {
        "id": "N4JRZip-LxGM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # Installing Java\n",
        "# !apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "# # Dowload Spark\n",
        "# !wget -q https://archive.apache.org/dist/spark/spark-3.2.1/spark-3.2.1-bin-hadoop2.7.tgz\n",
        "# # Decompressing archives\n",
        "# !tar xf spark-3.2.1-bin-hadoop2.7.tgz\n",
        "# # installing findspark\n",
        "# !pip install -q findspark\n",
        "# # installing pyspark\n",
        "# !pip install pyspark==3.2.1"
      ],
      "metadata": {
        "id": "g7uf9T1SAwMe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing os library\n",
        "# import os\n",
        "# # Java envirorment variable\n",
        "# os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "# # Spark envirorment variable\n",
        "# os.environ[\"SPARK_HOME\"] = \"/content/spark-3.1.2-bin-hadoop2.7\""
      ],
      "metadata": {
        "id": "V3ZS9Z2vYrxT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Better and consistent module that makes the same function as the code above"
      ],
      "metadata": {
        "id": "cTYBnobCPITD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyspark py4j"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "boHa2yWHE2TA",
        "outputId": "d0b19152-ad31-4a2a-d990-fac63e9676d0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pyspark\n",
            "  Downloading pyspark-3.3.1.tar.gz (281.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 281.4 MB 46 kB/s \n",
            "\u001b[?25hCollecting py4j\n",
            "  Downloading py4j-0.10.9.7-py2.py3-none-any.whl (200 kB)\n",
            "\u001b[K     |████████████████████████████████| 200 kB 50.6 MB/s \n",
            "\u001b[?25h  Downloading py4j-0.10.9.5-py2.py3-none-any.whl (199 kB)\n",
            "\u001b[K     |████████████████████████████████| 199 kB 9.8 MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.3.1-py2.py3-none-any.whl size=281845512 sha256=154bf6c929503e74cc017237b2189e7683c490559d40da3200ecf3c21f8d9c7b\n",
            "  Stored in directory: /root/.cache/pip/wheels/43/dc/11/ec201cd671da62fa9c5cc77078235e40722170ceba231d7598\n",
            "Successfully built pyspark\n",
            "Installing collected packages: py4j, pyspark\n",
            "Successfully installed py4j-0.10.9.5 pyspark-3.3.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Importing modules and pandas library that will be used later.**"
      ],
      "metadata": {
        "id": "z0lpamJ2PhzT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "import pyspark\n",
        "import pandas as pd\n",
        "# import findspark\n",
        "\n",
        "# findspark.init()"
      ],
      "metadata": {
        "id": "oLpozDIlCsnf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# instatiating spark instance\n",
        "spark = SparkSession.builder.master('local[*]').getOrCreate()\n",
        "\n",
        "spark"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 222
        },
        "id": "rLgNaJpJZWQf",
        "outputId": "48a36ef8-e232-4048-8aa8-68f4a21404c3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pyspark.sql.session.SparkSession at 0x7f158d1f6ee0>"
            ],
            "text/html": [
              "\n",
              "            <div>\n",
              "                <p><b>SparkSession - in-memory</b></p>\n",
              "                \n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://a8f4b099314f:4040\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v3.3.1</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>local[*]</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>pyspark-shell</code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        \n",
              "            </div>\n",
              "        "
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive', force_remount=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "70iuSZT29Tff",
        "outputId": "00e8db5c-5d14-43a8-fbaa-df86c609dccd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Next cells acquire parquet data from remote files, and transpose it to spark format"
      ],
      "metadata": {
        "id": "MEDLrMlgMaTM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fille = \"/content/gdrive/MyDrive/Data/Parquet/Formated/BrStocksFormated.parquet\"\n",
        "\n",
        "stocks = spark\\\n",
        "        .read.format(\"parquet\")\\\n",
        "        .option(\"inferSchema\", \"True\")\\\n",
        "        .option(\"header\",\"True\")\\\n",
        "        .parquet(fille)"
      ],
      "metadata": {
        "id": "pFHrGmPvaSTT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fille = \"/content/gdrive/MyDrive/Data/Parquet/Formated/Ebit.parquet\"\n",
        "\n",
        "ebit = spark\\\n",
        "        .read.format(\"parquet\")\\\n",
        "        .option(\"inferSchema\", \"True\")\\\n",
        "        .option(\"header\",\"True\")\\\n",
        "        .parquet(fille)"
      ],
      "metadata": {
        "id": "6zQPXfDcMtB5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fille = \"/content/gdrive/MyDrive/Data/Parquet/Formated/Stat.parquet\"\n",
        "\n",
        "stat = spark\\\n",
        "        .read.format(\"parquet\")\\\n",
        "        .option(\"inferSchema\", \"True\")\\\n",
        "        .option(\"header\",\"True\")\\\n",
        "        .parquet(fille)"
      ],
      "metadata": {
        "id": "8ZQgZzJ3NCB6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fille = \"/content/gdrive/MyDrive/Data/Parquet/Formated/Price.parquet\"\n",
        "\n",
        "price = spark\\\n",
        "        .read.format(\"parquet\")\\\n",
        "        .option(\"inferSchema\", \"True\")\\\n",
        "        .option(\"header\",\"True\")\\\n",
        "        .parquet(fille)"
      ],
      "metadata": {
        "id": "66vvcNGCCZoN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Creating Tables in order to manipulate data in SQL format"
      ],
      "metadata": {
        "id": "SubWtlFnNJHn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Stocks table\n",
        "stocks.createOrReplaceTempView(\"General\")\n",
        "# Ebit table\n",
        "ebit.createOrReplaceTempView(\"Ebit\")\n",
        "# Stat table\n",
        "stat.createOrReplaceTempView(\"Stat\")\n",
        "# Price table\n",
        "price.createOrReplaceTempView(\"Price\")"
      ],
      "metadata": {
        "id": "UD38ZEtovRHd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Note 1: \n",
        "'DetToEquity' must be in percentage, try to get it done here or otherwise in Power Bi\n",
        "\n",
        "---\n",
        "\n",
        "# Note 2: \n",
        "Find a way to calculate Beta values from \n",
        "historical data get it from other sources, but try to calculate it localy (Talk with Felipe, about the period of time that will be used as 'ytd' or montlhy). \n",
        "\n",
        "# Fixed:\n",
        "Beta values could be acquired trough yahoo finance"
      ],
      "metadata": {
        "id": "nv8S8_6bCqBk"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sKpxqfnyBnmn"
      },
      "source": [
        "Listing brazilian companies that will be researched and getting hystorical financial data\n",
        "filters that our Analyst indicates: Raking companies by: EBIT/EV > 0, ROE > 0, **Daily equity < 1.000.000.000**, revenueGrowth > 0 AND Will be done apart :(5-year-history) and all margins must be postive"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "DFT = spark.sql(\"\"\"\n",
        "SELECT e.Ticker,\n",
        "       Preco_Atual,\n",
        "       Fluxo_de_caixa_total,\n",
        "       Fluxo_de_caixa_por_acao,\n",
        "       EBITDA,\n",
        "       Divida_total,\n",
        "       Liquidez_imediata,\n",
        "       Liquidez_corrente,\n",
        "       Receita_total,\n",
        "       Divida_Patrimonio,\n",
        "       Receita_por_acao,\n",
        "       ROA,\n",
        "       ROE,\n",
        "       Lucro_Bruto,\n",
        "       Fluxo_de_Caixa_Livre,\n",
        "       Fluxo_de_caixa_operacional,\n",
        "       Crescimento_de_Receita_3T,\n",
        "       Margem_Bruta,\n",
        "       Margem_EBITIDA,\n",
        "       Margem_Operacional,\n",
        "       Margem_liquida,\n",
        "       Moeda,\n",
        "       Crescimento_dos_ganhos_3T,\n",
        "       EBIT,\n",
        "       Divida_Liquida\n",
        "FROM General g\n",
        "LEFT JOIN Ebit e ON g.Ticker = e.Ticker\n",
        "\"\"\").createOrReplaceTempView(\"Filtered\")"
      ],
      "metadata": {
        "id": "QQb5boETie0X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Stat = spark.sql(\"\"\"\n",
        "SELECT f.Ticker,\n",
        "       Preco_Atual,\n",
        "       Fluxo_de_caixa_total,\n",
        "       Fluxo_de_caixa_por_acao,\n",
        "       EBITDA,\n",
        "       Divida_total,\n",
        "       Liquidez_imediata,\n",
        "       Liquidez_corrente,\n",
        "       Receita_total,\n",
        "       Divida_Patrimonio,\n",
        "       Receita_por_acao,\n",
        "       ROA,\n",
        "       ROE,\n",
        "       Lucro_Bruto,\n",
        "       Fluxo_de_Caixa_Livre,\n",
        "       Fluxo_de_caixa_operacional,\n",
        "       Crescimento_de_Receita_3T,\n",
        "       Margem_Bruta,\n",
        "       Margem_EBITIDA,\n",
        "       Margem_Operacional,\n",
        "       Margem_liquida,\n",
        "       Moeda,\n",
        "       Crescimento_dos_ganhos_3T,\n",
        "       EBIT,\n",
        "       Divida_Liquida,\n",
        "      Beta,\n",
        "      Margem_de_Lucro,\n",
        "      Crescimento_de_receitas_4T,\n",
        "      Data_do_ultimo_dividendo,\n",
        "      Valor_do_ultimo_dividendo\n",
        "FROM Filtered f\n",
        "LEFT JOIN Stat s ON f.Ticker = s.Ticker;\n",
        "\"\"\").createOrReplaceTempView(\"Prstocks\")"
      ],
      "metadata": {
        "id": "u9cs56ygQ8fc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "DF = spark.sql(\"\"\"\n",
        "SELECT a.Ticker,\n",
        "       Preco_Atual,\n",
        "       Fluxo_de_caixa_total,\n",
        "       Fluxo_de_caixa_por_acao,\n",
        "       EBITDA,\n",
        "       Divida_total,\n",
        "       Liquidez_imediata,\n",
        "       Liquidez_corrente,\n",
        "       Receita_total,\n",
        "       Divida_Patrimonio,\n",
        "       Receita_por_acao,\n",
        "       ROA,\n",
        "       ROE,\n",
        "       Lucro_Bruto,\n",
        "       Fluxo_de_Caixa_Livre,\n",
        "       Fluxo_de_caixa_operacional,\n",
        "       Crescimento_de_Receita_3T,\n",
        "       Margem_Bruta,\n",
        "       Margem_EBITIDA,\n",
        "       Margem_Operacional,\n",
        "       Margem_liquida,\n",
        "       Moeda,\n",
        "       Crescimento_dos_ganhos_3T,\n",
        "       EBIT,\n",
        "       Divida_Liquida,\n",
        "       Beta,\n",
        "       Margem_de_Lucro,\n",
        "       Crescimento_de_receitas_4T,\n",
        "       Data_do_ultimo_dividendo,\n",
        "       Valor_do_ultimo_dividendo,\n",
        "       valor_de_mercado\n",
        "FROM Prstocks a\n",
        "LEFT JOIN Price p ON a.Ticker = p.Ticker;\n",
        "\"\"\").createOrReplaceTempView('Stocks')"
      ],
      "metadata": {
        "id": "GV26cS4ZCuWq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ev = número total de papeis x cotação + divida liquida\n",
        "ev = market value +  liquid debt \n",
        "\n",
        "PSR = PREÇO DA AÇÃO / RECEITA LIQUIDA POR AÇÃO "
      ],
      "metadata": {
        "id": "ZGP5as1CK1uo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "CALCULATE = spark.sql(\"\"\"\n",
        "SELECT * FROM Stocks\n",
        "WHERE Margem_Bruta > 0.0 AND Margem_EBITIDA > 0.0 AND \n",
        "Margem_Operacional > 0.0 AND Margem_liquida > 0.0 AND \n",
        "ROE > 0 AND Crescimento_de_Receita_3T > 0 AND \n",
        "((valor_de_mercado + Divida_Liquida)/EBIT) > 0.0;\n",
        "\"\"\")"
      ],
      "metadata": {
        "id": "_EHHjN73m9Zk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_final = CALCULATE.toPandas()"
      ],
      "metadata": {
        "id": "_KMykScpd0dc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_final.to_excel(\"/content/gdrive/MyDrive/Data/XLXS/Final/StocksFinal.xlsx\",\n",
        "                  sheet_name=\"Stocks\")"
      ],
      "metadata": {
        "id": "XLHH-FBIgAzy"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}