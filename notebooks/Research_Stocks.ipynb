{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/andrew3690/F-A_root_project/blob/Brazil/C%C3%B3pia_de_Research_Stocks.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N4JRZip-LxGM"
      },
      "source": [
        "# Downloading Modules:\n",
        "Spark, pyspark, Java.\n",
        "\n",
        "Bellow it is the old fashion way, risky not Recomended"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "g7uf9T1SAwMe"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "' \\n# # Installing Java\\n! apt-get install openjdk-8-jdk-headless -qq > /dev/null\\n# # Dowload Spark\\n! wget -q https://archive.apache.org/dist/spark/spark-3.2.1/spark-3.2.1-bin-hadoop2.7.tgz\\n# # Decompressing archives\\n! tar xf spark-3.2.1-bin-hadoop2.7.tgz\\n# # installing findspark\\n! pip install -q findspark\\n# # installing pyspark\\n! pip install pyspark==3.2.1\\n '"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\"\"\" \n",
        "# # Installing Java\n",
        "! apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "# # Dowload Spark\n",
        "! wget -q https://archive.apache.org/dist/spark/spark-3.2.1/spark-3.2.1-bin-hadoop2.7.tgz\n",
        "# # Decompressing archives\n",
        "! tar xf spark-3.2.1-bin-hadoop2.7.tgz\n",
        "# # installing findspark\n",
        "! pip install -q findspark\n",
        "# # installing pyspark\n",
        "! pip install pyspark==3.2.1\n",
        " \"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "V3ZS9Z2vYrxT"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "' \\n# Importing os library\\nimport os\\n# # Java envirorment variable\\nos.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\\n# # Spark envirorment variable\\nos.environ[\"SPARK_HOME\"] = \"/content/spark-3.1.2-bin-hadoop2.7\"\\n '"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\"\"\" \n",
        "# Importing os library\n",
        "import os\n",
        "# # Java envirorment variable\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "# # Spark envirorment variable\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.1.2-bin-hadoop2.7\"\n",
        " \"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cTYBnobCPITD"
      },
      "source": [
        "# Better and consistent library that makes the same function as the code above\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "boHa2yWHE2TA",
        "outputId": "2dd293d0-7923-4351-a6b7-66b27aea85f0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pyspark in c:\\users\\video\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (3.2.1)\n",
            "Requirement already satisfied: py4j in c:\\users\\video\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (0.10.9.3)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "[notice] A new release of pip available: 22.3.1 -> 23.0.1\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
          ]
        }
      ],
      "source": [
        "! pip install pyspark py4j"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z0lpamJ2PhzT"
      },
      "source": [
        "**Importing modules and pandas library that will be used later.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "oLpozDIlCsnf"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "import pyspark\n",
        "import pandas as pd\n",
        "# import findspark\n",
        "\n",
        "# findspark.init()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 219
        },
        "id": "rLgNaJpJZWQf",
        "outputId": "375d6ea4-ab28-428e-9c70-d6742f2396f9"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "            <div>\n",
              "                <p><b>SparkSession - in-memory</b></p>\n",
              "                \n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://DESKTOP-7VLQ9AP:4040\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v3.2.1</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>local[*]</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>pyspark-shell</code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        \n",
              "            </div>\n",
              "        "
            ],
            "text/plain": [
              "<pyspark.sql.session.SparkSession at 0x27e6e01a490>"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# instatianting spark instance\n",
        "spark = SparkSession.builder.master('local[*]').getOrCreate()\n",
        "\n",
        "spark"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Gooogle Colab Version"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "70iuSZT29Tff",
        "outputId": "7fa9d57b-f0c2-4c24-d1d3-841ff740eb4c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "\" \\nfrom google.colab import drive\\ndrive.mount('/content/gdrive', force_remount=True) \\n\""
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\"\"\" \n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive', force_remount=True) \n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MEDLrMlgMaTM"
      },
      "source": [
        "# Next cells acquire parquet data from remote files, and transpose it to spark format"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "pFHrGmPvaSTT"
      },
      "outputs": [],
      "source": [
        "fille = r\"C:/Users/video/Desktop/Projetos/API/F-A_root_project/notebooks/RAW_DATA/Parquet/BrStocksFormated.parquet\"\n",
        "\n",
        "stocks = spark\\\n",
        "        .read.format(\"parquet\")\\\n",
        "        .option(\"inferSchema\", \"True\")\\\n",
        "        .option(\"header\",\"True\")\\\n",
        "        .parquet(fille)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "6zQPXfDcMtB5"
      },
      "outputs": [],
      "source": [
        "fille = r\"C:/Users/video/Desktop/Projetos/API/F-A_root_project/notebooks/RAW_DATA/Parquet/Ebit.parquet\"\n",
        "\n",
        "ebit = spark\\\n",
        "        .read.format(\"parquet\")\\\n",
        "        .option(\"inferSchema\", \"True\")\\\n",
        "        .option(\"header\",\"True\")\\\n",
        "        .parquet(fille)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "8ZQgZzJ3NCB6"
      },
      "outputs": [],
      "source": [
        "fille = r\"C:/Users/video/Desktop/Projetos/API/F-A_root_project/notebooks/RAW_DATA/Parquet/Stat.parquet\"\n",
        "\n",
        "stat = spark\\\n",
        "        .read.format(\"parquet\")\\\n",
        "        .option(\"inferSchema\", \"True\")\\\n",
        "        .option(\"header\",\"True\")\\\n",
        "        .parquet(fille)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "66vvcNGCCZoN"
      },
      "outputs": [],
      "source": [
        "fille = r\"C:/Users/video/Desktop/Projetos/API/F-A_root_project/notebooks/RAW_DATA/Parquet/Price.parquet\"\n",
        "\n",
        "price = spark\\\n",
        "        .read.format(\"parquet\")\\\n",
        "        .option(\"inferSchema\", \"True\")\\\n",
        "        .option(\"header\",\"True\")\\\n",
        "        .parquet(fille)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [],
      "source": [
        "fille =r'C:/Users/video/Desktop/Projetos/API/F-A_root_project/notebooks/RAW_DATA/Parquet/Balance.parquet'\n",
        "\n",
        "balance = spark\\\n",
        "        .read.format(\"parquet\")\\\n",
        "        .option(\"inferSchema\", \"True\")\\\n",
        "        .option(\"header\",\"True\")\\\n",
        "        .parquet(fille)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SubWtlFnNJHn"
      },
      "source": [
        "Creating Tables in order to manipulate data in SQL format"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "UD38ZEtovRHd"
      },
      "outputs": [],
      "source": [
        "# Stocks table\n",
        "stocks.createOrReplaceTempView(\"General\")\n",
        "# Ebit table\n",
        "ebit.createOrReplaceTempView(\"Ebit\")\n",
        "# Stat table\n",
        "stat.createOrReplaceTempView(\"Stat\")\n",
        "# Price table\n",
        "price.createOrReplaceTempView(\"Price\")\n",
        "# Balance table\n",
        "balance.createOrReplaceTempView(\"Balance\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nv8S8_6bCqBk"
      },
      "source": [
        "# Note 1: \n",
        "'DetToEquity' must be in percentage, try to get it done here or otherwise in Power Bi\n",
        "\n",
        "---\n",
        "\n",
        "# Note 2: \n",
        "Find a way to calculate Beta values from \n",
        "historical data get it from other sources, but try to calculate it localy (Talk with Felipe, about the period of time that will be used as 'ytd' or montlhy). \n",
        "\n",
        "# Fixed:\n",
        "Beta values could be acquired trough yahoo finance"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sKpxqfnyBnmn"
      },
      "source": [
        "Listing brazilian companies that will be researched and getting hystorical financial data\n",
        "filters that our Analyst indicates: Raking companies by: EBIT/EV > 0, ROE > 0, **Daily equity < 1.000.000.000**, revenueGrowth > 0 AND Will be done apart :(5-year-history) and all margins must be postive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "_KbqxMcV6ACp"
      },
      "outputs": [],
      "source": [
        "# Already called at Calc Table\n",
        "Gen = spark.sql(\"\"\"\n",
        "SELECT * FROM General\n",
        "WHERE Moeda != 'USD' AND EBITDA != 0 AND ROA != 0 AND ROE != 0;\n",
        "\"\"\").createOrReplaceTempView(\"Gen\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "lf6g77UP6T4R"
      },
      "outputs": [],
      "source": [
        "#Gen.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "FjPkiCwx8xeo"
      },
      "outputs": [],
      "source": [
        "# Already called at RSCH table\n",
        "Ebit = spark.sql(\"\"\"\n",
        "SELECT * FROM Ebit\n",
        "where Divida_Liquida != 0;\n",
        "\"\"\").createOrReplaceTempView(\"Debt\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "UBBFDO-Z89_w"
      },
      "outputs": [],
      "source": [
        "#Ebit.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "RapgMFkNEhWt"
      },
      "outputs": [],
      "source": [
        "# Already called at RSCH table\n",
        "Price = spark.sql(\"\"\"\n",
        "SELECT * FROM Price\n",
        "WHERE valor_de_mercado != 0;\n",
        "\"\"\")#.createOrReplaceTempView(\"Prices\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "u3wQByiGE9qO"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+--------+----------------+-----------------+-----------------+\n",
            "|  Ticker|valor_de_mercado|volume_de_mercado|__index_level_0__|\n",
            "+--------+----------------+-----------------+-----------------+\n",
            "|ZAMP3.SA|      1290876288|                0|                0|\n",
            "|AERI3.SA|       964650304|                0|                3|\n",
            "|DOTZ3.SA|       113162640|                0|                5|\n",
            "|MLAS3.SA|      1763443968|                0|                8|\n",
            "|NINJ3.SA|       134109408|                0|               11|\n",
            "|MODL3.SA|      1179109888|                0|               14|\n",
            "|VITT3.SA|      1594519040|                0|               15|\n",
            "|KRSA3.SA|       874195328|                0|               16|\n",
            "|ALLD3.SA|       470764032|                0|               23|\n",
            "|ATMP3.SA|        27776526|                0|               25|\n",
            "|JSLG3.SA|      1982957056|                0|               27|\n",
            "|ELMD3.SA|      1802294016|                0|               29|\n",
            "|OPCT3.SA|       617591488|                0|               31|\n",
            "|WEST3.SA|       113666680|                0|               32|\n",
            "|CSED3.SA|      1016417792|                0|               33|\n",
            "|BMOB3.SA|      1232547456|                0|               34|\n",
            "|MBLY3.SA|       340768000|                0|               36|\n",
            "|ESPA3.SA|       433707616|                0|               37|\n",
            "|NGRD3.SA|       274267648|                0|               40|\n",
            "|AVLL3.SA|       219868096|                0|               41|\n",
            "+--------+----------------+-----------------+-----------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "Price.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "abGJeNp68atZ"
      },
      "source": [
        "Alternative operations, while dealing with a little amount of data, and it can be joined here"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "qa3_DcgM9cFg"
      },
      "outputs": [],
      "source": [
        "Stat = spark.sql(\"\"\"\n",
        "SELECT * FROM Stat\n",
        "WHERE Beta != 0.5;\n",
        "\"\"\")#.createOrReplaceTempView(\"Stat\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Stat.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "_EHHjN73m9Zk"
      },
      "outputs": [],
      "source": [
        "CALCULATE = spark.sql(\"\"\"\n",
        "SELECT * FROM Gen\n",
        "WHERE Margem_Bruta > 0.0 AND Margem_EBITIDA > 0.0 AND \n",
        "Margem_Operacional > 0.0 AND Margem_liquida > 0.0 AND \n",
        "ROE > 0 AND Crescimento_de_Receita_3T > 0;\n",
        "\"\"\")#.createOrReplaceTempView(\"Calc\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BLS_f98nmjji",
        "outputId": "48485c91-f0eb-4f67-a848-4b06613ed014"
      },
      "outputs": [],
      "source": [
        "#CALCULATE.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "uGV8lO4pNWcT"
      },
      "outputs": [],
      "source": [
        "RSCH = spark.sql(\"\"\"\n",
        "SELECT p.Ticker, p.valor_de_mercado, p.volume_de_mercado, e.EBIT, e.Divida_Liquida FROM Price as p, Ebit as e\n",
        "WHERE (p.valor_de_mercado + e.Divida_Liquida)/e.EBIT > 0.0 AND \n",
        "p.Ticker == e.Ticker; \n",
        "\"\"\")#.createOrReplaceTempView(\"RSCH\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {},
      "outputs": [],
      "source": [
        "Balance = spark.sql(\"\"\"\n",
        "SELECT * FROM Balance\n",
        "\"\"\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QQb5boETie0X"
      },
      "outputs": [],
      "source": [
        "# DFT = spark.sql(\"\"\"\n",
        "# SELECT e.Ticker,\n",
        "#        Preco_Atual,\n",
        "#        Fluxo_de_caixa_total,\n",
        "#        Fluxo_de_caixa_por_acao,\n",
        "#        EBITDA,\n",
        "#        Divida_total,\n",
        "#        Liquidez_imediata,\n",
        "#        Liquidez_corrente,\n",
        "#        Receita_total,\n",
        "#        Divida_Patrimonio,\n",
        "#        Receita_por_acao,\n",
        "#        ROA,\n",
        "#        ROE,\n",
        "#        Lucro_Bruto,\n",
        "#        Fluxo_de_Caixa_Livre,\n",
        "#        Fluxo_de_caixa_operacional,\n",
        "#        Crescimento_de_Receita_3T,\n",
        "#        Margem_Bruta,\n",
        "#        Margem_EBITIDA,\n",
        "#        Margem_Operacional,\n",
        "#        Margem_liquida,\n",
        "#        Moeda,\n",
        "#        Crescimento_dos_ganhos_3T,\n",
        "#        EBIT,\n",
        "#        Divida_Liquida\n",
        "# FROM General g\n",
        "# LEFT JOIN Ebit e ON g.Ticker = e.Ticker\n",
        "# \"\"\").createOrReplaceTempView(\"Filtered\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u9cs56ygQ8fc"
      },
      "outputs": [],
      "source": [
        "# Stat = spark.sql(\"\"\"\n",
        "# SELECT f.Ticker,\n",
        "#        Preco_Atual,\n",
        "#        Fluxo_de_caixa_total,\n",
        "#        Fluxo_de_caixa_por_acao,\n",
        "#        EBITDA,\n",
        "#        Divida_total,\n",
        "#        Liquidez_imediata,\n",
        "#        Liquidez_corrente,\n",
        "#        Receita_total,\n",
        "#        Divida_Patrimonio,\n",
        "#        Receita_por_acao,\n",
        "#        ROA,\n",
        "#        ROE,\n",
        "#        Lucro_Bruto,\n",
        "#        Fluxo_de_Caixa_Livre,\n",
        "#        Fluxo_de_caixa_operacional,\n",
        "#        Crescimento_de_Receita_3T,\n",
        "#        Margem_Bruta,\n",
        "#        Margem_EBITIDA,\n",
        "#        Margem_Operacional,\n",
        "#        Margem_liquida,\n",
        "#        Moeda,\n",
        "#        Crescimento_dos_ganhos_3T,\n",
        "#        EBIT,\n",
        "#        Divida_Liquida,\n",
        "#       Beta,\n",
        "#       Margem_de_Lucro,\n",
        "#       Crescimento_de_receitas_4T,\n",
        "#       Valor_do_ultimo_dividendo\n",
        "# FROM Filtered f\n",
        "# LEFT JOIN Stat s ON f.Ticker = s.Ticker;\n",
        "# \"\"\").createOrReplaceTempView(\"Prstocks\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GV26cS4ZCuWq"
      },
      "outputs": [],
      "source": [
        "# DF = spark.sql(\"\"\"\n",
        "# SELECT a.Ticker,\n",
        "#        Preco_Atual,\n",
        "#        Fluxo_de_caixa_total,\n",
        "#        Fluxo_de_caixa_por_acao,\n",
        "#        EBITDA,\n",
        "#        Divida_total,\n",
        "#        Liquidez_imediata,\n",
        "#        Liquidez_corrente,\n",
        "#        Receita_total,\n",
        "#        Divida_Patrimonio,\n",
        "#        Receita_por_acao,\n",
        "#        ROA,\n",
        "#        ROE,\n",
        "#        Lucro_Bruto,\n",
        "#        Fluxo_de_Caixa_Livre,\n",
        "#        Fluxo_de_caixa_operacional,\n",
        "#        Crescimento_de_Receita_3T,\n",
        "#        Margem_Bruta,\n",
        "#        Margem_EBITIDA,\n",
        "#        Margem_Operacional,\n",
        "#        Margem_liquida,\n",
        "#        Moeda,\n",
        "#        Crescimento_dos_ganhos_3T,\n",
        "#        EBIT,\n",
        "#        Divida_Liquida,\n",
        "#        Beta,\n",
        "#        Margem_de_Lucro,\n",
        "#        Crescimento_de_receitas_4T,\n",
        "#        Valor_do_ultimo_dividendo,\n",
        "#        valor_de_mercado\n",
        "# FROM Prstocks a\n",
        "# LEFT JOIN Price p ON a.Ticker = p.Ticker;\n",
        "# \"\"\").createOrReplaceTempView('Stocks')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZGP5as1CK1uo"
      },
      "source": [
        "ev = número total de papeis x cotação + divida liquida\n",
        "ev = market value +  liquid debt \n",
        "\n",
        "PSR = PREÇO DA AÇÃO / RECEITA LIQUIDA POR AÇÃO "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_KMykScpd0dc"
      },
      "outputs": [],
      "source": [
        "# df_final = CALCULATE.toPandas()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XLHH-FBIgAzy"
      },
      "outputs": [],
      "source": [
        "# df_final.to_excel(\"/content/gdrive/MyDrive/Data/XLSX/Final/StocksFinal.xlsx\",\n",
        "#                   sheet_name=\"Stocks\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7MJkJf8SguRE"
      },
      "source": [
        "Pandas setting DataFrame to pandas format:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "-FFxq6uLe49F"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\video\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:194: FutureWarning: Passing unit-less datetime64 dtype to .astype is deprecated and will raise in a future version. Pass 'datetime64[ns]' instead\n",
            "  series = series.astype(t, copy=False)\n",
            "c:\\Users\\video\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[column_name] = series\n",
            "c:\\Users\\video\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[column_name] = series\n",
            "c:\\Users\\video\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[column_name] = series\n",
            "c:\\Users\\video\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[column_name] = series\n",
            "c:\\Users\\video\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[column_name] = series\n",
            "c:\\Users\\video\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[column_name] = series\n",
            "c:\\Users\\video\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[column_name] = series\n",
            "c:\\Users\\video\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[column_name] = series\n",
            "c:\\Users\\video\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[column_name] = series\n",
            "c:\\Users\\video\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[column_name] = series\n",
            "c:\\Users\\video\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[column_name] = series\n",
            "c:\\Users\\video\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[column_name] = series\n",
            "c:\\Users\\video\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[column_name] = series\n",
            "c:\\Users\\video\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[column_name] = series\n",
            "c:\\Users\\video\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[column_name] = series\n",
            "c:\\Users\\video\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[column_name] = series\n",
            "c:\\Users\\video\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[column_name] = series\n",
            "c:\\Users\\video\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[column_name] = series\n",
            "c:\\Users\\video\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[column_name] = series\n",
            "c:\\Users\\video\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[column_name] = series\n",
            "c:\\Users\\video\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[column_name] = series\n",
            "c:\\Users\\video\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[column_name] = series\n",
            "c:\\Users\\video\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[column_name] = series\n",
            "c:\\Users\\video\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[column_name] = series\n",
            "c:\\Users\\video\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[column_name] = series\n",
            "c:\\Users\\video\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[column_name] = series\n",
            "c:\\Users\\video\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[column_name] = series\n",
            "c:\\Users\\video\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[column_name] = series\n",
            "c:\\Users\\video\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[column_name] = series\n",
            "c:\\Users\\video\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[column_name] = series\n",
            "c:\\Users\\video\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[column_name] = series\n",
            "c:\\Users\\video\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[column_name] = series\n",
            "c:\\Users\\video\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[column_name] = series\n",
            "c:\\Users\\video\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[column_name] = series\n",
            "c:\\Users\\video\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[column_name] = series\n",
            "c:\\Users\\video\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[column_name] = series\n",
            "c:\\Users\\video\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[column_name] = series\n",
            "c:\\Users\\video\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[column_name] = series\n",
            "c:\\Users\\video\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[column_name] = series\n",
            "c:\\Users\\video\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[column_name] = series\n",
            "c:\\Users\\video\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[column_name] = series\n",
            "c:\\Users\\video\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[column_name] = series\n",
            "c:\\Users\\video\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[column_name] = series\n"
          ]
        }
      ],
      "source": [
        "df_stat = Stat.toPandas()\n",
        "\n",
        "df_calc = CALCULATE.toPandas()\n",
        "\n",
        "df_rsc = RSCH.toPandas()\n",
        "\n",
        "df_balance = Balance.toPandas()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "BNc_zBG8j3zL"
      },
      "outputs": [],
      "source": [
        "df_stat.to_excel(r\"C:/Users/video/Desktop/Projetos/API/F-A_root_project/notebooks/Final/Stat.xlsx\",\n",
        "                 sheet_name=\"stat\")\n",
        "\n",
        "df_calc.to_excel(r\"C:/Users/video/Desktop/Projetos/API/F-A_root_project/notebooks/Final/Calcl.xlsx\",\n",
        "                 sheet_name=\"calc\")\n",
        "\n",
        "df_rsc.to_excel(r\"C:/Users/video/Desktop/Projetos/API/F-A_root_project/notebooks/Final/RSCH.xlsx\",\n",
        "                sheet_name=\"research\")\n",
        "\n",
        "df_balance.to_excel(r\"C:/Users/video/Desktop/Projetos/API/F-A_root_project/notebooks/Final/Balance.xlsx\",\n",
        "                sheet_name=\"balance\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.1"
    },
    "vscode": {
      "interpreter": {
        "hash": "598092329803f2218a82426b846daf764474eee9678032408d5a767cd4823601"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
